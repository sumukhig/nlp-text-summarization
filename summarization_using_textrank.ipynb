{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from rouge_score import rouge_scorer\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_abstract(element):\n",
    "    abstract = ''\n",
    "    if element.tag == 'ABSTRACT':\n",
    "        for child in element:\n",
    "            line = child.text \n",
    "            # Text preprocessing\n",
    "            line = line.lower()\n",
    "            line = line.replace(\"quot\", \"\")\n",
    "            line = re.sub(r'https?://\\S+', '', line) \n",
    "            line = re.sub(r'[^\\w\\s]', '', line)\n",
    "            # Append to abstract\n",
    "            abstract = abstract + '\\n' + line\n",
    "    \n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract paper contents from the XML tree\n",
    "def extract_data(element):\n",
    "    text_data = (element.text or \"\").strip()\n",
    "    # Separate abstract text from rest of the paper\n",
    "    if element.tag != 'ABSTRACT':\n",
    "        # Text preprocessing\n",
    "        text_data = text_data.lower()\n",
    "        text_data = text_data.replace(\"quot\", \"\")\n",
    "        text_data = re.sub(r'https?://\\S+', '', text_data) \n",
    "        text_data = re.sub(r'[^\\w\\s]', '', text_data)\n",
    "\n",
    "        for child in element:\n",
    "            text_data = np.append(text_data, extract_data(child))  \n",
    "\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents(document_path):\n",
    "    summary_path = glob.glob(document_path + '/summary/*.txt')[0]\n",
    "    article_path = glob.glob(document_path + '/Documents_xml/*.xml')[0]\n",
    "    \n",
    "    summary_file = open(summary_path)\n",
    "    summary_content = summary_file.read()\n",
    "\n",
    "    article_tree = ET.parse(article_path)\n",
    "    article_root = article_tree.getroot()\n",
    "\n",
    "    abstract_element = None\n",
    "\n",
    "    for child in article_root:\n",
    "        if child.tag == 'ABSTRACT':\n",
    "            abstract_element = child\n",
    "\n",
    "    sentences = extract_data(article_root)\n",
    "\n",
    "    sentences = [s for s in list(sentences) if s != '']\n",
    "\n",
    "    title = sentences[0]\n",
    "\n",
    "    sentences = sentences[1:]\n",
    "\n",
    "    sentence_embeddings = embedder.encode(sentences)\n",
    "\n",
    "    if abstract_element: \n",
    "        abstract = extract_abstract(abstract_element)\n",
    "        abstract = title + abstract \n",
    "    else:\n",
    "        abstract = []\n",
    "\n",
    "    contents = {'title': title, 'sentences': sentences, 'embeddings': sentence_embeddings, 'summary': summary_content, 'abstract': abstract}\n",
    "\n",
    "    document_name = document_path.split('/')[-1]\n",
    "\n",
    "    # data[document_name] = contents\n",
    "    \n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sim_matrix(contents):\n",
    "    sentences = contents['sentences']\n",
    "    sentence_embeddings = contents['embeddings']\n",
    "    # similarity matrix\n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])   \n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sentence_embeddings[i].reshape(1,768), sentence_embeddings[j].reshape(1,768))[0,0] \n",
    "                # sim_mat[i][j] = len([word for word in sentences[i].split() if word in sentences[j].split()])/(np.log(len(sentences[i])) + np.log(len(sentences[i]))\n",
    "    \n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textRank(sim_mat, sentences):\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    return ranked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(title, ranked_sentences, n):\n",
    "    result = title\n",
    "    for i in range(n):\n",
    "        # print(f\"{i}.{ranked_sentences[i][1]}\")\n",
    "        result = result + '\\n ' + ranked_sentences[i][1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reference_summaries, generated_summaries):\n",
    "    # Use Rouge score technique to evaluate text summary\n",
    "    rouge_scoring = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "\n",
    "    score_dict = {  \"r1-precision\":[]\n",
    "                   , \"r1-recall\":[]\n",
    "                   , \"r1-f1-score\":[]\n",
    "                   , \"r2-precision\":[]\n",
    "                   , \"r2-recall\":[]\n",
    "                   , \"r2-f1-score\":[]\n",
    "                   , \"rL-precision\":[]\n",
    "                   , \"rL-recall\":[]\n",
    "                   , \"rL-f1-score\":[]\n",
    "                    }\n",
    "\n",
    "    for i, (reference_summary, summary) in enumerate(zip(reference_summaries, generated_summaries)):\n",
    "        scores = rouge_scoring.score(reference_summary, summary)\n",
    "        # print(f\"ROUGE scores for generated summary {i+1}:\")\n",
    "        # print(\"ROUGE-1 Precision:\", scores['rouge1'].precision)\n",
    "        # print(\"ROUGE-1 Recall:\", scores['rouge1'].recall)\n",
    "        # print(\"ROUGE-1 F1-Score:\", scores['rouge1'].fmeasure)\n",
    "        # print(\"ROUGE-2 Precision:\", scores['rouge2'].precision)\n",
    "        # print(\"ROUGE-2 Recall:\", scores['rouge2'].recall)\n",
    "        # print(\"ROUGE-2 F1-Score:\", scores['rouge2'].fmeasure)\n",
    "        # print(\"ROUGE-L Precision:\", scores['rougeL'].precision)\n",
    "        # print(\"ROUGE-L Recall:\", scores['rougeL'].recall)\n",
    "        # print(\"ROUGE-L F1-Score:\", scores['rougeL'].fmeasure)\n",
    "        # print(\"---------------------\")\n",
    "\n",
    "        score_dict[\"r1-precision\"].append(scores['rouge1'].precision)\n",
    "        score_dict[\"r1-recall\"].append(scores['rouge1'].recall)\n",
    "        score_dict[\"r1-f1-score\"].append(scores['rouge1'].fmeasure)\n",
    "        score_dict[\"r2-precision\"].append(scores['rouge2'].precision)\n",
    "        score_dict[\"r2-recall\"].append(scores['rouge2'].recall)\n",
    "        score_dict[\"r2-f1-score\"].append(scores['rouge2'].fmeasure)\n",
    "        score_dict[\"rL-precision\"].append(scores['rougeL'].precision)\n",
    "        score_dict[\"rL-recall\"].append(scores['rougeL'].recall)\n",
    "        score_dict[\"rL-f1-score\"].append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    return score_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory='./data/scisummnet_release1.1__20190413/top1000_complete'\n",
    "embedder = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "data = {}\n",
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = []\n",
    "manual_summaries = []\n",
    "generated_summaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ./data/scisummnet_release1.1__20190413/top1000_complete/P99-1008 123 Duration: 4.377468824386597\n",
      "2 ./data/scisummnet_release1.1__20190413/top1000_complete/J93-2003 677 Duration: 97.07724070549011\n",
      "3 ./data/scisummnet_release1.1__20190413/top1000_complete/W08-0336 184 Duration: 8.469384908676147\n",
      "4 ./data/scisummnet_release1.1__20190413/top1000_complete/N10-1119 157 Duration: 6.893887996673584\n",
      "5 ./data/scisummnet_release1.1__20190413/top1000_complete/P04-3022 77 Duration: 1.919508934020996\n",
      "6 ./data/scisummnet_release1.1__20190413/top1000_complete/J06-3003 600 Duration: 70.17219305038452\n",
      "7 ./data/scisummnet_release1.1__20190413/top1000_complete/W08-0309 244 Duration: 13.543020963668823\n",
      "8 ./data/scisummnet_release1.1__20190413/top1000_complete/P08-1114 145 Duration: 6.138139963150024\n",
      "9 ./data/scisummnet_release1.1__20190413/top1000_complete/P87-1033 139 Duration: 5.61949610710144\n",
      "10 ./data/scisummnet_release1.1__20190413/top1000_complete/J98-1006 338 Duration: 24.171996116638184\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(glob.glob(data_directory + '/*')[0:10]):\n",
    "    start_time = time.time()\n",
    "    contents = get_contents(doc)\n",
    "    sim_mat = generate_sim_matrix(contents)\n",
    "    ranked_sentences = textRank(sim_mat, contents['sentences'])\n",
    "    result = generate_summary(contents['title'], ranked_sentences, n)\n",
    "    abstracts.append(contents['abstract'])\n",
    "    manual_summaries.append(contents['summary'])\n",
    "    generated_summaries.append(result)\n",
    "    end_time = time.time()\n",
    "    print(i+1, doc, len(contents['sentences']), 'Duration:', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 F1-Score: 0.3410811142281667\n",
      "Average ROUGE-1 F1-Score: 0.2973768448016149\n"
     ]
    }
   ],
   "source": [
    "rouge_scores_manual_summaries = evaluate(manual_summaries, generated_summaries)\n",
    "rouge_scores_abstracts = evaluate(abstracts, generated_summaries)\n",
    "\n",
    "print('Average ROUGE-1 F1-Score:', mean(rouge_scores_manual_summaries['r1-f1-score']))\n",
    "print('Average ROUGE-1 F1-Score:', mean(rouge_scores_abstracts['r1-f1-score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways to improve:\n",
    "\n",
    "1 Remove abstract from paper content\n",
    "2 Remove stop words\n",
    "3 Rearrange sentences in summary according to their order of appearance in the paper\n",
    "4 Use different embedding techniques\n",
    "5 Use other weight calculations such as isf-modified-cosine-similarity\n",
    "6 Understand the relationship between ROUGE scores and length of the summary and set n accordingly\n",
    "7 Find other evaluation metrics to compare\n",
    "8 Stem words in the summaries before comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
