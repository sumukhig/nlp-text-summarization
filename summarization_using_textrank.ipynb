{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from rouge_score import rouge_scorer\n",
    "from statistics import mean\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract abstract from XML tree\n",
    "def extract_abstract(element):\n",
    "    abstract = ''\n",
    "    if element.tag == 'ABSTRACT':\n",
    "        for child in element:\n",
    "            line = child.text \n",
    "            # Text preprocessing\n",
    "            line = line.lower()\n",
    "            line = line.replace(\"quot\", \"\")\n",
    "            line = re.sub(r'https?://\\S+', '', line) \n",
    "            line = re.sub(r'[^\\w\\s]', '', line)\n",
    "            # Append to abstract\n",
    "            abstract = abstract + '\\n' + line\n",
    "    \n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract paper text from XML tree\n",
    "def extract_data(element):\n",
    "    text_data = (element.text or \"\").strip()\n",
    "    # Separate abstract text from rest of the paper\n",
    "    if element.tag != 'ABSTRACT':\n",
    "        # Text preprocessing\n",
    "        text_data = text_data.lower()\n",
    "        text_data = text_data.replace(\"quot\", \"\")\n",
    "        text_data = re.sub(r'https?://\\S+', '', text_data) \n",
    "        text_data = re.sub(r'[^\\w\\s]', '', text_data)\n",
    "\n",
    "        for child in element:\n",
    "            text_data = np.append(text_data, extract_data(child))  \n",
    "\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_embeddings(sentences):\n",
    "    clean_sentences = []\n",
    "    for snt in list(sentences):\n",
    "        clean_snt = [word for word in snt.split() if word not in stop_words]\n",
    "        clean_sentences.append(\" \".join(clean_snt))\n",
    "    \n",
    "\n",
    "    sentence_embeddings = embedder.encode(clean_sentences)\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract all data related to a paper from dataset\n",
    "def get_contents(document_path):\n",
    "    summary_path = glob.glob(document_path + '/summary/*.txt')[0]\n",
    "    article_path = glob.glob(document_path + '/Documents_xml/*.xml')[0]\n",
    "    \n",
    "    summary_file = open(summary_path)\n",
    "    summary_content = summary_file.read()\n",
    "\n",
    "    article_tree = ET.parse(article_path)\n",
    "    article_root = article_tree.getroot()\n",
    "\n",
    "    abstract_element = None\n",
    "\n",
    "    for child in article_root:\n",
    "        if child.tag == 'ABSTRACT':\n",
    "            abstract_element = child\n",
    "\n",
    "    sentences = extract_data(article_root)\n",
    "\n",
    "    sentences = [s for s in list(sentences) if s != '']\n",
    "\n",
    "    title = sentences[0]\n",
    "\n",
    "    sentences = sentences[1:]\n",
    "\n",
    "    sentence_embeddings = generate_sentence_embeddings(sentences)\n",
    "\n",
    "    if abstract_element: \n",
    "        abstract = extract_abstract(abstract_element)\n",
    "        abstract = title + abstract \n",
    "    else:\n",
    "        abstract = ''\n",
    "\n",
    "    contents = {'title': title, 'sentences': sentences, 'embeddings': sentence_embeddings, 'summary': summary_content, 'abstract': abstract}\n",
    "\n",
    "    document_name = document_path.split('/')[-1]\n",
    "\n",
    "    data[document_name] = contents\n",
    "    \n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the similarity matrix of the paper\n",
    "def generate_sim_matrix(contents):\n",
    "    sentences = contents['sentences']\n",
    "    sentence_embeddings = contents['embeddings']\n",
    "    # similarity matrix\n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])   \n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sentence_embeddings[i].reshape(1,768), sentence_embeddings[j].reshape(1,768))[0,0] \n",
    "                # sim_mat[i][j] = len([word for word in sentences[i].split() if word in sentences[j].split()])/(np.log(len(sentences[i])) + np.log(len(sentences[i]))\n",
    "    \n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute textRank of sentences in the paper\n",
    "def textRank(sim_mat, sentences):\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    return ranked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a summary using top ranking sentences\n",
    "def generate_summary(title, sentences, ranked_sentences, n):\n",
    "    sentence_positions = []\n",
    "    result = title\n",
    "    for i in range(n):\n",
    "        # print(f\"{i}.{ranked_sentences[i][1]}\")\n",
    "        sentence_positions.append(list(sentences).index(ranked_sentences[i][1]))\n",
    "    \n",
    "    sentence_positions_sorted = sorted(sentence_positions)\n",
    "    \n",
    "    for pos in sentence_positions_sorted:\n",
    "        result = result + '\\n' + sentences[pos]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model performance\n",
    "def evaluate(ls_documents, reference_summaries, generated_summaries):\n",
    "    # Use Rouge score technique to evaluate text summary\n",
    "    rouge_scoring = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "\n",
    "    score_dict = {   \"document\":[]\n",
    "                   , \"r1-precision\":[]\n",
    "                   , \"r1-recall\":[]\n",
    "                   , \"r1-f1-score\":[]\n",
    "                   , \"r2-precision\":[]\n",
    "                   , \"r2-recall\":[]\n",
    "                   , \"r2-f1-score\":[]\n",
    "                   , \"rL-precision\":[]\n",
    "                   , \"rL-recall\":[]\n",
    "                   , \"rL-f1-score\":[]\n",
    "                    }\n",
    "\n",
    "    for i, (reference_summary, summary) in enumerate(zip(reference_summaries, generated_summaries)):\n",
    "        score_dict['document'].append(ls_documents[i].split('/')[-1])\n",
    "        if reference_summary == '':\n",
    "            score_dict[\"r1-precision\"].append(0)\n",
    "            score_dict[\"r1-recall\"].append(0)\n",
    "            score_dict[\"r1-f1-score\"].append(0)\n",
    "            score_dict[\"r2-precision\"].append(0)\n",
    "            score_dict[\"r2-recall\"].append(0)\n",
    "            score_dict[\"r2-f1-score\"].append(0)\n",
    "            score_dict[\"rL-precision\"].append(0)\n",
    "            score_dict[\"rL-recall\"].append(0)\n",
    "            score_dict[\"rL-f1-score\"].append(0)\n",
    "        else:\n",
    "            scores = rouge_scoring.score(reference_summary, summary)\n",
    "            # print(f\"ROUGE scores for generated summary {i+1}:\")\n",
    "            # print(\"ROUGE-1 Precision:\", scores['rouge1'].precision)\n",
    "            # print(\"ROUGE-1 Recall:\", scores['rouge1'].recall)\n",
    "            # print(\"ROUGE-1 F1-Score:\", scores['rouge1'].fmeasure)\n",
    "            # print(\"ROUGE-2 Precision:\", scores['rouge2'].precision)\n",
    "            # print(\"ROUGE-2 Recall:\", scores['rouge2'].recall)\n",
    "            # print(\"ROUGE-2 F1-Score:\", scores['rouge2'].fmeasure)\n",
    "            # print(\"ROUGE-L Precision:\", scores['rougeL'].precision)\n",
    "            # print(\"ROUGE-L Recall:\", scores['rougeL'].recall)\n",
    "            # print(\"ROUGE-L F1-Score:\", scores['rougeL'].fmeasure)\n",
    "            # print(\"---------------------\")\n",
    "\n",
    "            score_dict[\"r1-precision\"].append(scores['rouge1'].precision)\n",
    "            score_dict[\"r1-recall\"].append(scores['rouge1'].recall)\n",
    "            score_dict[\"r1-f1-score\"].append(scores['rouge1'].fmeasure)\n",
    "            score_dict[\"r2-precision\"].append(scores['rouge2'].precision)\n",
    "            score_dict[\"r2-recall\"].append(scores['rouge2'].recall)\n",
    "            score_dict[\"r2-f1-score\"].append(scores['rouge2'].fmeasure)\n",
    "            score_dict[\"rL-precision\"].append(scores['rougeL'].precision)\n",
    "            score_dict[\"rL-recall\"].append(scores['rougeL'].recall)\n",
    "            score_dict[\"rL-f1-score\"].append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    return score_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sumukhiganesan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Set input data path and initialize\n",
    "data_directory='./data/scisummnet_release1.1__20190413/top1000_complete'\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# embedder = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "embedder = SentenceTransformer('allenai-specter')\n",
    "\n",
    "data = {}\n",
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to receive outputs\n",
    "abstracts = []\n",
    "manual_summaries = []\n",
    "generated_summaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ./data/scisummnet_release1.1__20190413/top1000_complete/P99-1008 123 Duration: 5.086650848388672\n",
      "2 ./data/scisummnet_release1.1__20190413/top1000_complete/J93-2003 677 Duration: 96.30519914627075\n",
      "3 ./data/scisummnet_release1.1__20190413/top1000_complete/W08-0336 184 Duration: 9.461012840270996\n",
      "4 ./data/scisummnet_release1.1__20190413/top1000_complete/N10-1119 157 Duration: 8.185255289077759\n"
     ]
    }
   ],
   "source": [
    "# Iterate over papers in the dataset to generate sumamries\n",
    "ls_documents = glob.glob(data_directory + '/*')\n",
    "\n",
    "for i, doc in enumerate(ls_documents):\n",
    "    start_time = time.time()\n",
    "    contents = get_contents(doc)\n",
    "    sim_mat = generate_sim_matrix(contents)\n",
    "    ranked_sentences = textRank(sim_mat, contents['sentences'])\n",
    "    result = generate_summary(contents['title'], contents['sentences'], ranked_sentences, n)\n",
    "    abstracts.append(contents['abstract'])\n",
    "    manual_summaries.append(contents['summary'])\n",
    "    generated_summaries.append(result)\n",
    "    end_time = time.time()\n",
    "    print(i+1, doc, len(contents['sentences']), 'Duration:', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Scores against Manual Summaries:\n",
      "ROUGE-1 Precision: 0.314546479579676\n",
      "ROUGE-1 Recall: 0.48383904695638025\n",
      "ROUGE-1 F1-Score: 0.3741649709834971\n",
      "ROUGE-2 Precision: 0.08426490588009021\n",
      "ROUGE-2 Recall: 0.1310524035713424\n",
      "ROUGE-2 F1-Score: 0.10076841415076362\n",
      "ROUGE-L Precision: 0.1593792301188855\n",
      "ROUGE-L Recall: 0.24640929771990466\n",
      "ROUGE-L F1-Score: 0.18987792745554333\n",
      "-------------------------------------\n",
      "Average Scores against Abstracts:\n",
      "ROUGE-1 Precision: 0.2596313600433187\n",
      "ROUGE-1 Recall: 0.5039695340501792\n",
      "ROUGE-1 F1-Score: 0.333268400604966\n",
      "ROUGE-2 Precision: 0.07250590708509119\n",
      "ROUGE-2 Recall: 0.14135966098222424\n",
      "ROUGE-2 F1-Score: 0.0932611894909891\n",
      "ROUGE-L Precision: 0.14475528574142496\n",
      "ROUGE-L Recall: 0.2855501792114695\n",
      "ROUGE-L F1-Score: 0.18672112517721687\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "rouge_scores_manual_summaries = evaluate(ls_documents, manual_summaries, generated_summaries)\n",
    "rouge_scores_abstracts = evaluate(ls_documents, abstracts, generated_summaries)\n",
    "\n",
    "\n",
    "# Display scores\n",
    "print('Average Scores against Manual Summaries:')\n",
    "print('ROUGE-1 Precision:', mean(rouge_scores_manual_summaries['r1-precision']))\n",
    "print('ROUGE-1 Recall:', mean(rouge_scores_manual_summaries['r1-recall']))\n",
    "print('ROUGE-1 F1-Score:', mean(rouge_scores_manual_summaries['r1-f1-score']))\n",
    "print('ROUGE-2 Precision:', mean(rouge_scores_manual_summaries['r2-precision']))\n",
    "print('ROUGE-2 Recall:', mean(rouge_scores_manual_summaries['r2-recall']))\n",
    "print('ROUGE-2 F1-Score:', mean(rouge_scores_manual_summaries['r2-f1-score']))\n",
    "print('ROUGE-L Precision:', mean(rouge_scores_manual_summaries['rL-precision']))\n",
    "print('ROUGE-L Recall:', mean(rouge_scores_manual_summaries['rL-recall']))\n",
    "print('ROUGE-L F1-Score:', mean(rouge_scores_manual_summaries['rL-f1-score']))\n",
    "\n",
    "print('-------------------------------------')\n",
    "\n",
    "print('Average Scores against Abstracts:')\n",
    "print('ROUGE-1 Precision:', mean(rouge_scores_abstracts['r1-precision']))\n",
    "print('ROUGE-1 Recall:', mean(rouge_scores_abstracts['r1-recall']))\n",
    "print('ROUGE-1 F1-Score:', mean(rouge_scores_abstracts['r1-f1-score']))\n",
    "print('ROUGE-2 Precision:', mean(rouge_scores_abstracts['r2-precision']))\n",
    "print('ROUGE-2 Recall:', mean(rouge_scores_abstracts['r2-recall']))\n",
    "print('ROUGE-2 F1-Score:', mean(rouge_scores_abstracts['r2-f1-score']))\n",
    "print('ROUGE-L Precision:', mean(rouge_scores_abstracts['rL-precision']))\n",
    "print('ROUGE-L Recall:', mean(rouge_scores_abstracts['rL-recall']))\n",
    "print('ROUGE-L F1-Score:', mean(rouge_scores_abstracts['rL-f1-score']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scores\n",
    "\n",
    "df1 = pd.DataFrame(rouge_scores_manual_summaries)\n",
    "df1.to_csv('./rouge_scores_manual_summaries.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rouge_scores_abstracts)\n",
    "df2.to_csv('./rouge_scores_abstracts.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
