# -*- coding: utf-8 -*-
"""NLP-Project-Group10-FineTunedBART.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GSScOzTduQQy8k9E0mbG8du6KpSfTPZu
"""

! pip install -U sentence_transformers

! pip install transformers datasets rouge-score

"""# Step 1: Preprocess the data"""

import torch
from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments
import os
import xml.etree.ElementTree as ET
import re
import numpy as np

# Function to extract all text data from the XML tree
def extract_text_data(element):

    # data cleaning steps
    text = (element.text or "").strip()
    text_data = text.lower()
    text_data = text_data.replace("quot", "")
    text_data = re.sub(r'https?://\S+', '', text_data)
    text_data = re.sub(r'[^\w\s]', '', text_data)

    for child in element:
        text_data = np.append(text_data, extract_text_data(child))

    return text_data

def extract_data_from_xml(xml_file_path):
    tree = ET.parse(xml_file_path)
    root = tree.getroot()
    text_data = extract_text_data(root)
    return " ".join(text_data)

def extract_text_from_summary(summary_file_path):
    with open(summary_file_path, 'r') as file:
        return file.read()

def preprocess_data(directory_path):
    articles = []
    summaries = []
    for folder in os.listdir(directory_path):
        article_folder = os.path.join(directory_path, folder)
        if os.path.isdir(article_folder):
            xml_path = os.path.join(article_folder, "Documents_xml", f"{folder}.xml")
            summary_path = os.path.join(article_folder, "summary", f"{folder}.gold.txt")

            # Extract article and summary
            article = extract_data_from_xml(xml_path)
            summary = extract_text_from_summary(summary_path)

            articles.append(article)
            summaries.append(summary)
    return articles, summaries

articles, summaries = preprocess_data("/content/drive/MyDrive/top1000_complete")

"""# Step 2: Load pre-trained BART and tokenizer"""

model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')

# Tokenize our data
train_encodings = tokenizer(articles, truncation=True, padding='max_length', max_length=512)
train_labels = tokenizer(summaries, truncation=True, padding='max_length', max_length=150)

# Create a DataLoader
class SummaryDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels['input_ids'][idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = SummaryDataset(train_encodings, train_labels)

"""# Step 3: Train the BART model"""

training_args = TrainingArguments(
    output_dir='./results',
    per_device_train_batch_size=4,
    num_train_epochs=3,
    logging_dir='./logs',
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    tokenizer=tokenizer
)

trainer.train()

"""# Step 4: Save and evaluate"""

model.save_pretrained("./fine_tuned_bart")